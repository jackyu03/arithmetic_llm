# STAT 359 Arithmetic LLM Project Proposal

This project aims to elevate the capabilities of a specialized Arithmetic Large Language Model by addressing fundamental limitations in its current architecture, training methodology, and interpretability. The first phase of improvement focuses on modernizing the Transformer backbone to align with modern LLM design. We will transition from standard absolute positional embeddings to Rotary Positional Embeddings (RoPE), allowing the model to generalize better to longer arithmetic sequences than those seen during training. Additionally, replacing LayerNorm with RMSNorm will improve training stability, while implementing KV caching will drastically reduce the computational complexity of inference from quadratic to linear time, enabling faster experimentation. Concurrently, the tokenizer will be refined to handle digits and whitespace more consistently, preventing the arbitrary merging of numbers that currently hampers the model's ability to parse complex expressions.

To transcend simple pattern matching and instill robust logic, we will implement a contrastive learning training strategy. Instead of training solely on correct solution paths, this method introduces a contrastive learning framework where the model is exposed to pairs of correct and incorrect reasoning chains. By training the model to explicitly critique and distinguish between valid logic and common arithmetic fallacies (effectively acting as a teacher correcting a student), we force the network to internalize the boundaries of mathematical rules rather than just memorizing successful templates. This moves the concept from a passive user feature to an active training objective, resulting in a model that is far less prone to hallucinating plausible-sounding but mathematically invalid steps. In addition, we will implement “curriculum” learning, where the model is exposed to simpler examples (like “1+2=3”) to more complex ones (“5/4*7”) gradually, which resembles a student finishing an entire curriculum.

Apart from using the conventional metrics, we will validate these improvements through a “Mind Reader” interpretability mechanism. This visualization tool will expose the model's internal attention weights during inference, creating a real-time heatmap that shows exactly which input numbers the model is focusing on at each step of the calculation. This ensures that the model is genuinely attending to the relevant operands (e.g., focusing on "10" and "3" when calculating "10-3") rather than relying on spurious correlations in the dataset.

Finally, we will conduct a rigorous ablation study on Group Relative Policy Optimization (GRPO) reward signals as suggested by domain research. We will move beyond binary correctness rewards to test granular signals, including step-wise verification, length penalties to discourage verbosity, and template-matching rewards. This study will document the causal relationship between specific reward structures and the emergence of “nonsense” logic, ensuring the final model balances accuracy with coherent, human-readable reasoning.